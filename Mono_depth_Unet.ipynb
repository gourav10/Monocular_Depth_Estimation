{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0d1da-b799-412d-bdf7-0b535175ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90416a42-2a2d-4d97-af83-76917673bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from urllib.request import FancyURLopener\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0921738-d209-47f8-bfc9-399d706b3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import exp\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca29c35-7a53-4fbc-9895-facafa6748b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812c874-9842-4e3a-b444-0dd984b7a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from genericpath import isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa04276-6413-49ec-8d95-eecb8b194580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms as T\n",
    "from torch import optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f82371-74d4-45d4-976d-c3f30955559c",
   "metadata": {},
   "source": [
    "# Download and Setup of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7318bf8-8e61-45ae-877a-6e9f826d4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, destination, tmp_dir='/tmp'):\n",
    "    def _progress(count, block_size, total_size):\n",
    "        sys.stdout.write('\\rDownloading %s %.1f%%' % (url,\n",
    "          float(count * block_size) / float(total_size) * 100.0))\n",
    "        sys.stdout.flush()\n",
    "    urlretrieve = FancyURLopener().retrieve\n",
    "    if url.endswith('.zip'):\n",
    "        local_zip_path = os.path.join(tmp_dir, 'datasets_download.zip')\n",
    "        urlretrieve(url, local_zip_path, _progress)\n",
    "        with zipfile.ZipFile(local_zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall('/content/nyu_depth_v2/temp')\n",
    "        os.remove(local_zip_path)\n",
    "    else:\n",
    "        urlretrieve(url, destination, _progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c32b6b-3d26-44ee-9732-639abad99d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_download(url, destination):\n",
    "    if not os.path.isfile(destination):\n",
    "        download(url, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da62cc7-cde2-4f42-b173-928591c70175",
   "metadata": {},
   "outputs": [],
   "source": [
    "NYUD_URL = 'http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat'\n",
    "NYUD_SPLITS_URL = 'http://horatio.cs.nyu.edu/mit/silberman/indoor_seg_sup/splits.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7cfc1-3cbd-4175-8f6a-5affc9fcb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_nyu_depth_v2_dataset(source_dir, target_dir):\n",
    "    if not os.path.isdir(source_dir):\n",
    "        os.makedirs(source_dir)\n",
    "    nyud_file_path = os.path.join(source_dir, 'nyu_depth_v2_labeled.mat')\n",
    "    splits_file_path = os.path.join(source_dir, 'splits.mat')\n",
    "\n",
    "    nyud_gdrive_file_path = './dataset/nyu_depth_v2_labeled.mat'\n",
    "    splits_gdrivefile_path = './dataset/splits.mat'\n",
    "\n",
    "    if(not os.path.isfile(nyud_gdrive_file_path) and not not os.path.isfile(splits_gdrivefile_path)):\n",
    "        dataset_download(NYUD_URL, nyud_file_path)\n",
    "        dataset_download(NYUD_SPLITS_URL, splits_file_path)\n",
    "    else:\n",
    "        return nyud_gdrive_file_path,splits_gdrivefile_path\n",
    "      \n",
    "    return nyud_file_path,splits_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e9c42-2cfe-46c1-b5e7-a1c22c955449",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyud_file_path,splits_file_path = save_nyu_depth_v2_dataset('./content/nyu_depth_v2','./content/nyu_depth_v2/labeled')\n",
    "print(\"{}\\n{}\".format(nyud_file_path,splits_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ed1df-15c7-481d-bc4d-d0fd56aa5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(source_dir, target_dir):\n",
    "    print(\"Loading dataset: NYU Depth V2\")\n",
    "    nyud_dict = h5py.File(nyud_file_path, 'r')\n",
    "    splits_dict = scipy.io.loadmat(splits_file_path)\n",
    "    return nyud_dict, splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f96fa1-575f-4d85-9b57-3abbc5cdc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = './content/nyu_depth_v2/'\n",
    "nyud_dict, splits_dict = get_dataset(nyud_file_path,target_dir)\n",
    "pprint.pprint(nyud_dict.keys())\n",
    "images = np.asarray(nyud_dict['images'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5b70e-cc70-43f3-bcd4-7bbbb0792699",
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.asarray(nyud_dict['depths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(depths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5388955",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_indicies = int(795*(1-0.2))\n",
    "print(train_split_indicies)\n",
    "print()\n",
    "pprint.pprint(len(splits_dict['trainNdxs'][:train_split_indicies, 0] - 1))\n",
    "pprint.pprint(len(splits_dict['trainNdxs'][train_split_indicies:795, 0] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077d21d-1026-42ef-a239-02bd2323d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_split(splits_dict,dataset_size = 100,split_percent = 0.2):\n",
    "    indices = splits_dict['trainNdxs'][:, 0] - 1\n",
    "    train_split_indicies = int(dataset_size*(1-split_percent))\n",
    "\n",
    "    train_indices = splits_dict['trainNdxs'][:, 0] - 1\n",
    "    print(\"Training Data Size: \",len(indices[:]))\n",
    "\n",
    "    validation_indices = splits_dict['testNdxs'][:, 0] - 1\n",
    "    print(\"Validation Data Size: \",len(validation_indices))\n",
    "    \n",
    "#     test_indices = splits_dict['testNdxs'][:, 0] - 1\n",
    "#     print(\"Testing Data Size: \",len(test_indices))\n",
    "    return train_indices, validation_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd35969-de74-4e82-8184-ea2ce4c1f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, validation_indices = get_train_validation_split(splits_dict,dataset_size = 795,split_percent = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f5445-7028-4ca5-855a-f45c74b316b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_depths(images,depths, train_indices):\n",
    "    images_train = np.take(images, train_indices, axis=0)\n",
    "    images_train = images_train.swapaxes(2, 3)\n",
    "    print(images_train.shape)\n",
    "\n",
    "    depths_train = np.expand_dims(depths.swapaxes(1, 2), 1)\n",
    "    depths_train = np.take(depths_train, train_indices, axis=0)\n",
    "    print(depths_train.shape)\n",
    "    return images_train,depths_train\n",
    "\n",
    "train_images, train_depths = get_images_depths(images,depths,train_indices)\n",
    "validation_images, validation_depths = get_images_depths(images,depths,validation_indices)\n",
    "#test_images,test_depths = get_images_depths(images,depths,test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ab25a-509c-47aa-9d33-72d9b6365712",
   "metadata": {},
   "source": [
    "# Visualization of Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520346ce-44db-44f8-a495-4753d8ecfb44",
   "metadata": {},
   "source": [
    "**Training Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859b756-fac5-4b3b-87f1-2b5808de3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_images))\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "k=1\n",
    "for sample_idx in range(8):\n",
    "    print(\"Data Type: {}, Pre-Transpose: {}\".format(type(train_images[sample_idx]),train_images[sample_idx].shape))\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(train_images[sample_idx].transpose(1,2,0),interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(train_depths[sample_idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbf2ef-e1a1-4c9f-9937-3f2a99101bbb",
   "metadata": {},
   "source": [
    "**Test Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8e9c1-9db0-436b-9aea-e9cb95ccd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_images))\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "k=1\n",
    "for sample_idx in range(7):\n",
    "    print(\"Data Type: {}, Pre-Transpose: {}\".format(type(validation_images[sample_idx]),validation_images[sample_idx].shape))\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(validation_images[sample_idx].transpose(1,2,0),cmap='gray',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(validation_depths[sample_idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0560c4-1bc8-44f0-ab64-841a0624b3b0",
   "metadata": {},
   "source": [
    "# Generate Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8fcde-10ca-4dcb-b795-91f1ab0149e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.transforms.functional import hflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501706af-a953-45a2-aa85-e223846c768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUDepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,images,indices,depths,transform=None,train=True):\n",
    "        self.images = images\n",
    "        self.indicies = indices\n",
    "        self.maps = depths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image = torch.from_numpy(self.images[index]).float().div(255)\n",
    "        dmap = torch.from_numpy(self.maps[index]).float().div(255)*1000\n",
    "        dmap = torch.clamp(dmap, 10, 1000)\n",
    "        \n",
    "        if(self.transform):\n",
    "            image = self.transform(image)\n",
    "            dmap = self.transform(dmap)\n",
    "        if random.random() > 0.5:\n",
    "            image = hflip(image)\n",
    "            #image = image[[2,1,0],:,:]\n",
    "            dmap = hflip(dmap)\n",
    "            \n",
    "        return image,dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bce67a-d189-40c7-8020-1be3638254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "train_dataset = NYUDepthDataset(train_images,train_indices,train_depths,transform = transforms.Compose([T.Resize((320,320))]))\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=2,shuffle=False)\n",
    "\n",
    "\n",
    "validation_dataset = NYUDepthDataset(validation_images,validation_indices,validation_depths,transform = transforms.Compose([T.Resize((320,320))]))\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=2,shuffle=True)\n",
    "\n",
    "# test_dataset = NYUDepthDataset(test_images,test_indices,test_depths,transform = transforms.Compose([T.Resize((320,320))]))\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0b9dd-d7f2-4fa7-a15e-60f512f7e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for batch_idx,(features,targets) in enumerate(train_loader):\n",
    "    counter+=1\n",
    "print(\"Total Number of Batches: \",counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140287b-e2fc-4259-9e26-6fb9e59ca1bb",
   "metadata": {},
   "source": [
    "**Visualize Train Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be02339-3b2f-4f64-92d8-0f3ce3fdfa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data,dmap = next(examples)\n",
    "# print(example_data[0].shape)\n",
    "# print(dmap[0].shape)\n",
    "k=1\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for idx in range(2):\n",
    "    plt.subplot(5,4, k)\n",
    "    img = example_data.numpy()\n",
    "    dmap_n = dmap.numpy()\n",
    "    plt.imshow(img[idx].transpose(1,2,0),interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(dmap_n[idx].transpose(1,2,0),cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "valexamples = iter(validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273634c-fa83-4902-a58e-500d7dd97298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_data,example_targets = next(examples)\n",
    "k=1\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "for idx in range(2):\n",
    "    plt.subplot(5,4, k)\n",
    "    img = example_data.numpy()\n",
    "    plt.imshow(img[idx].transpose(1,2,0),cmap='gray',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(example_targets[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73e876-a1a0-480b-97cf-523b8f279138",
   "metadata": {},
   "source": [
    "# Building U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2031b-7e49-47be-8764-4abf2d068bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_c,out_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_c,out_c,kernel_size=3,padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_c,out_c,kernel_size=3,padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4433cc-70f1-49b0-8f4d-27635356e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(tensor,target_tensor):\n",
    "    target_size = target_tensor.size()[2]\n",
    "    tensor_size = tensor.size()[2]\n",
    "    delta = tensor_size-target_size\n",
    "    delta = delta//2\n",
    "    return tensor[:,:,delta:tensor_size-delta,delta:tensor_size-delta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885db64-c2dd-410d-bcde-a6e79859eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet,self).__init__()\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.down_conv_1 = double_conv(3,64)\n",
    "        self.down_conv_2 = double_conv(64,128)\n",
    "        self.down_conv_3 = double_conv(128,256)\n",
    "        self.down_conv_4 = double_conv(256,512)\n",
    "        self.down_conv_5 = double_conv(512,1024)\n",
    "\n",
    "        self.up_trans_1 = nn.ConvTranspose2d(\n",
    "            in_channels=1024,\n",
    "            out_channels=512,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "    \n",
    "        self.up_conv_1 = double_conv(1024,512)\n",
    "\n",
    "        self.up_trans_2 = nn.ConvTranspose2d(\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "    \n",
    "        self.up_conv_2 = double_conv(512,256)\n",
    "\n",
    "        self.up_trans_3 = nn.ConvTranspose2d(\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        \n",
    "        self.up_conv_3 = double_conv(256,128)\n",
    "    \n",
    "        self.up_trans_4 = nn.ConvTranspose2d(\n",
    "            in_channels=128,\n",
    "            out_channels=64,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "        \n",
    "        self.up_conv_4 = double_conv(128,64)\n",
    "\n",
    "        self.out = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=1,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self,image):\n",
    "        #encoder\n",
    "        x1 = self.down_conv_1(image) #\n",
    "        x2 = self.max_pool_2x2(x1)\n",
    "        x3 = self.down_conv_2(x2) #\n",
    "        x4 = self.max_pool_2x2(x3) \n",
    "        x5 = self.down_conv_3(x4) # \n",
    "        x6 = self.max_pool_2x2(x5) \n",
    "        x7 = self.down_conv_4(x6) #\n",
    "        x8 = self.max_pool_2x2(x7)\n",
    "        x9 = self.down_conv_5(x8)\n",
    "        x10 = self.max_pool_2x2(x9)\n",
    "        #decoder\n",
    "        x = self.up_trans_1(x9)\n",
    "        y = crop_image(x7,x)\n",
    "        x = self.up_conv_1(torch.cat([x,y],1))\n",
    "\n",
    "        x = self.up_trans_2(x)\n",
    "        y = crop_image(x5,x)\n",
    "        x = self.up_conv_2(torch.cat([x,y],1))\n",
    "\n",
    "        x = self.up_trans_3(x)\n",
    "        y = crop_image(x3,x)\n",
    "        x = self.up_conv_3(torch.cat([x,y],1))\n",
    "\n",
    "        x = self.up_trans_4(x)\n",
    "        y = crop_image(x1,x)\n",
    "        x = self.up_conv_4(torch.cat([x,y],1))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644dc000-066d-420d-ac15-db0cb1a83a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "summary(model.to('cuda'),(3,320,320))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00172f02-4489-4825-a1da-7c322e1e38af",
   "metadata": {},
   "source": [
    "**Model Output before Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a465414",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c82b09-debb-4e3c-8b48-1bcaed79c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "example_data,example_targets = next(examples)\n",
    "output = model(example_data)\n",
    "k=1\n",
    "fig = plt.figure(figsize=(21,10))\n",
    "for idx in range(2):\n",
    "    plt.subplot(2,6, k)\n",
    "    image_num = example_data.numpy()\n",
    "    plt.imshow(image_num[idx].transpose(1,2,0),cmap='gray',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2,6, k)\n",
    "    image_num = example_data.numpy()\n",
    "    plt.imshow(example_targets[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2,6, k)\n",
    "    output_num = output.to('cpu').detach().numpy()\n",
    "    print(output.shape)\n",
    "    plt.imshow(output_num[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e1dde-c703-4274-8262-01f6f0f2053a",
   "metadata": {},
   "source": [
    "# Define Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc4135-1cdc-4fb9-b076-580931728425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af99feb-5a26-49d7-a5df-6199911bb02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(window_size, channel=1):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7447f0-fd31-4309-8a80-bffc2062fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(img1, img2, val_range, window_size=11, window=None, size_average=True, full=False):\n",
    "    L = val_range\n",
    "\n",
    "    padd = 0\n",
    "    (_, channel, height, width) = img1.size()\n",
    "    if window is None:\n",
    "        real_size = min(window_size, height, width)\n",
    "        window = create_window(real_size, channel=channel).to(img1.device)\n",
    "\n",
    "    mu1 = F.conv2d(img1, window, padding=padd, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=padd, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=padd, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=padd, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=padd, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = (0.01 * L) ** 2\n",
    "    C2 = (0.03 * L) ** 2\n",
    "\n",
    "    v1 = 2.0 * sigma12 + C2\n",
    "    v2 = sigma1_sq + sigma2_sq + C2\n",
    "    cs = torch.mean(v1 / v2)  # contrast sensitivity\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * v1) / ((mu1_sq + mu2_sq + C1) * v2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_map.mean()\n",
    "    else:\n",
    "        ret = ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "    if full:\n",
    "        return ret, cs\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb34df1-ceb6-43d5-9de7-5e8448124618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caecffe-8d3e-4133-9feb-afc9bd7b25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loss(gen_frames, gt_frames, alpha=1):\n",
    "\n",
    "    def gradient(x):\n",
    "        # idea from tf.image.image_gradients(image)\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n",
    "        # x: (b,c,h,w), float32 or float64\n",
    "        # dx, dy: (b,c,h,w)\n",
    "\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        # gradient step=1\n",
    "        left = x\n",
    "        right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
    "        top = x\n",
    "        bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
    "\n",
    "        # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
    "        dx, dy = right - left, bottom - top \n",
    "        # dx will always have zeros in the last column, right-left\n",
    "        # dy will always have zeros in the last row,    bottom-top\n",
    "        dx[:, :, :, -1] = 0\n",
    "        dy[:, :, -1, :] = 0\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "    # gradient\n",
    "    gen_dx, gen_dy = gradient(gen_frames)\n",
    "    gt_dx, gt_dy = gradient(gt_frames)\n",
    "    #\n",
    "    grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
    "    grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379ba90-8655-49be-a89d-eb94ac4975ce",
   "metadata": {},
   "source": [
    "# Train and Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee4fe6-63c0-4faa-998a-95906ac0059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if(torch.cuda.is_available()):\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2d1e5-eeba-484f-8b9c-42058365e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_dmap(images,dmap):\n",
    "    fig = plt.figure(figsize=(21,10))\n",
    "    plt.subplot(2,2, 1)\n",
    "    plt.imshow(image_num[0].transpose(1,2,0),interpolation='none')\n",
    "    plt.subplot(2,2, 2)\n",
    "    plt.imshow(dmap[0][0],cmap='plasma',interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ee70c-316f-481d-9061-a0609d668207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data_loader, optimizer, criterion, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for batch_idx, (feature_data,labels) in tqdm(enumerate(data_loader)):\n",
    "        feature_data = feature_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        depth_n = DepthNorm(labels,1000.0)\n",
    "        output = model(feature_data)\n",
    "        l_depth = criterion(output, depth_n)\n",
    "        l_ssim = torch.clamp((1 - ssim(output, depth_n, val_range = 1000.0 / 10.0)) * 0.5, 0, 1)\n",
    "        l_grad = gradient_loss(output,depth_n)\n",
    "        loss = (0.01*l_grad) + (1.0 * l_ssim) + (0.1 * l_depth)\n",
    "        loss = (1.0 * l_ssim) + (0.1 * l_depth)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += labels.size(0)        \n",
    "        # train_loss += loss.data.item()\n",
    "        train_loss += (1/(batch_idx+1))*(loss.item()/feature_data.size(0) - train_loss)\n",
    "        num_batches+=1\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd15164-35a8-4a9c-b2b0-931cf2c0046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,data_loader, optimizer, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    num_batches = 0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    for batch_idx,(feature_data,labels) in tqdm(enumerate(data_loader)):\n",
    "        feature_data = feature_data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        output = model(feature_data)\n",
    "        loss = criterion(output,labels)\n",
    "        total += labels.size(0)\n",
    "        # val_loss += loss.item()\n",
    "        val_loss = (1/(batch_idx+1))*(loss.item()/feature_data.size(0) - val_loss)\n",
    "        num_batches+=1\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3787f-a236-4331-9645-b2c8d075f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(num_epochs,train_losses,test_losses):\n",
    "    \n",
    "    # Using Numpy to create an array X\n",
    "    X = range(num_epochs)\n",
    "    \n",
    "    # Assign variables to the y axis part of the curve\n",
    "    y = train_losses\n",
    "    z = test_losses\n",
    "    \n",
    "    plt.plot(X,y,color='blue')\n",
    "    plt.plot(X,z,color='red')\n",
    "    plt.legend(['Train Loss','Validation Loss'],loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(\"Training and Validation Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6cd86-78ef-4fbc-8e02-7d6882ccae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model,num_epochs,train_dl,validation_dl,best_model):\n",
    "    test_losses=[]\n",
    "    test_accuracies = []\n",
    "    train_losses=[]\n",
    "    train_accuracies=[]\n",
    "    \n",
    "    min_loss=float('inf')\n",
    "    min_train_loss = float('inf')\n",
    "    learn_rate = 0.01\n",
    "    \n",
    "    device = get_device()\n",
    "    #criterion = torch.nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_train_loss = train(model,train_dl, optimizer, criterion, device)\n",
    "        test_loss = validate(model,validation_dl, optimizer, criterion, device)\n",
    "        \n",
    "        # if(num_epochs%10==0):\n",
    "        #   if(epoch%10==0):\n",
    "        #     print(\"Epoch:{}, Train Loss: {:.4f}, Valid Loss: {:.4f}\".format(epoch,train_loss,test_loss))\n",
    "        # else:\n",
    "        print(\"Epoch:{}/{}, Running Train Loss: {:.4f},Valid Loss: {:.4f}\".format(epoch,num_epochs,running_train_loss,test_loss))\n",
    "\n",
    "        train_losses.append(running_train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if(min_loss>test_loss):\n",
    "            best_model = copy.deepcopy(model)\n",
    "            min_loss = test_loss\n",
    "            min_train_loss = running_train_loss\n",
    "            print(\"Saving Best Model with Min Validation Loss: \", min_loss)\n",
    "    \n",
    "    plot_loss(num_epochs,train_losses,test_losses)\n",
    "    torch.save(best_model,'mono_depth_unet.pt')\n",
    "    return best_model,min_loss,min_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133cad3-d6ce-4128-a088-93472fb61a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "model = UNet()\n",
    "best_model = model\n",
    "best_model,train_loss,val_loss = fit_model(model,num_epochs,train_loader,validation_loader,best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48116e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'mono_depth_unet_ep12.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762687c-ca14-48c0-8bcd-1f975058e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('mono_depth_unet.pt')\n",
    "best_model.to('cpu')\n",
    "best_model.eval()\n",
    "examples = enumerate(validation_loader)\n",
    "batch_idx, (example_data,example_targets) = next(examples)\n",
    "output = best_model(example_data)\n",
    "k=1\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "for idx in range(2):\n",
    "    plt.subplot(2,6, k)\n",
    "    image_num = example_data.numpy()\n",
    "    plt.imshow(image_num[idx].transpose(1,2,0),cmap='gray',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(2,6, k)\n",
    "    plt.imshow(example_targets[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(2,6, k)\n",
    "    output_num = output.to('cpu').detach().numpy()\n",
    "    print(output.shape)\n",
    "    plt.imshow(output_num[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c632e-44de-473c-be20-50da93069dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('mono_depth_unet_ep12.pt')\n",
    "best_model.to('cpu')\n",
    "best_model.eval()\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data,example_targets) = next(examples)\n",
    "output = best_model(example_data)\n",
    "k=1\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "for idx in range(2):\n",
    "    plt.subplot(2,6, k)\n",
    "    image_num = example_data.numpy()\n",
    "    plt.imshow(image_num[idx].transpose(1,2,0),cmap='gray',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(2,6, k)\n",
    "    plt.imshow(example_targets[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(2,6, k)\n",
    "    output_num = output.to('cpu').detach().numpy()\n",
    "    print(output.shape)\n",
    "    plt.imshow(output_num[idx][0],cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
