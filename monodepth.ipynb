{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTdG1dsOkueZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import h5py\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x27xLvEc56zc",
    "outputId": "5aed0d7b-07e6-4a16-b538-f367cc49bf70"
   },
   "outputs": [],
   "source": [
    "nyud_file_path, splits_file_path = './data/nyu_depth_v2_labeled.mat','./data/splits.mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYHgoFenYGdt"
   },
   "outputs": [],
   "source": [
    "def get_dataset(source_dir, target_dir):\n",
    "    print(\"Loading dataset: NYU Depth V2\")\n",
    "    nyud_dict = h5py.File(nyud_file_path, 'r')\n",
    "    splits_dict = io.loadmat(splits_file_path)\n",
    "    return nyud_dict, splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nw__HRVnlBDr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r5FrHa2DSKTi",
    "outputId": "4d26b62c-49ea-4593-df6b-acdb3ee8c84b"
   },
   "outputs": [],
   "source": [
    "target_dir = '/content/nyu_depth_v2/'\n",
    "nyud_dict, splits_dict = get_dataset(nyud_file_path,target_dir)\n",
    "pprint.pprint(nyud_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(nyud_dict['images'])\n",
    "images = images.swapaxes(2, 3)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkRy-9ogqlds"
   },
   "outputs": [],
   "source": [
    "depths = np.asarray(nyud_dict['depths'])\n",
    "depths = depths.swapaxes(1, 2)\n",
    "depths = np.expand_dims(depths, 1)\n",
    "depths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrJkM1qyn2b0",
    "outputId": "fb6770ba-dbb2-4197-e89c-17d8484e658d"
   },
   "outputs": [],
   "source": [
    "train_indices = splits_dict['trainNdxs'][:, 0] - 1\n",
    "print(\"Training Data Size: \", len(train_indices))\n",
    "test_indices = splits_dict['testNdxs'][:, 0] - 1\n",
    "print(\"Testing Data Size: \", len(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Klc4QPgiqsBy"
   },
   "outputs": [],
   "source": [
    "#train_images = np.take(images, train_indices, axis=0)\n",
    "#test_images = np.take(images, test_indices, axis=0)\n",
    "\n",
    "#print(train_images.shape)\n",
    "\n",
    "#train_depths = np.take(depths, train_indices, axis=0)\n",
    "#test_depths = np.take(depths, test_indices, axis=0)\n",
    "\n",
    "#print(train_depths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFtaKRzpHicq"
   },
   "source": [
    "**Training Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PVVVgO0pj8MQ",
    "outputId": "acfc9f6f-0e9d-49a0-bdc6-7ce5bbc9ce59"
   },
   "outputs": [],
   "source": [
    "print(len(images))\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "k=1\n",
    "for sample_idx in range(7):\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(images[sample_idx].transpose((1,2,0)), interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.subplot(5,4, k)\n",
    "    plt.imshow(depths[sample_idx].transpose((1,2,0)), cmap='plasma',interpolation='none')\n",
    "    k+=1\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVZkQiYb46hB"
   },
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import hflip\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NYUDepthDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, maps, transform=None):\n",
    "        self.images = images\n",
    "        self.maps = maps\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = torch.from_numpy(self.images[index]).float().div(255)\n",
    "        dmap = torch.from_numpy(self.maps[index]).float()#.div(255) #* 1000\n",
    "        #dmap = torch.clamp(dmap, 10, 1000)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            dmap = self.transform(dmap)\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            image = hflip(image)\n",
    "            image = image[[2, 1, 0], :, :]\n",
    "            dmap = hflip(dmap)\n",
    "        \n",
    "        return image, dmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = NYUDepthDataset(np.take(images, train_indices, axis=0), np.take(depths, train_indices, axis=0))\n",
    "# val_data = NYUDepthDataset(np.take(images, test_indices, axis=0), np.take(depths, test_indices, axis=0))\n",
    "\n",
    "train_data = NYUDepthDataset(np.take(images, np.arange(0, 1000), axis=0), np.take(depths, np.arange(0, 1000), axis=0))\n",
    "#val_data = NYUDepthDataset(np.take(images, np.arange(1000, 1449), axis=0), np.take(depths, np.arange(1000, 1449), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, dmap = next(iter(train_data))\n",
    "img, dmap = img.numpy(), dmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, dmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img.transpose((1,2,0)), interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(dmap.transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=2,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, dmap = next(iter(train_loader))\n",
    "img, dmap = img.numpy(), dmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, dmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img[0].transpose((1,2,0)), interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(dmap[0].transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(name, pretrained=True):\n",
    "    if name == 'densenet169':\n",
    "         backbone = models.densenet169(pretrained=True).features\n",
    "    elif name == 'resnet50':\n",
    "        backbone = models.resnet50(pretrained=pretrained)\n",
    "    else:\n",
    "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
    "    \n",
    "    if name.startswith('densenet'):\n",
    "        feature_names = [None, 'relu0', 'denseblock1', 'denseblock2', 'denseblock3']\n",
    "        backbone_output = 'denseblock4'\n",
    "    elif name.startswith('resnet'):\n",
    "        feature_names = [None, 'relu', 'layer1', 'layer2', 'layer3']\n",
    "        backbone_output = 'layer4'\n",
    "        \n",
    "    return backbone, feature_names, backbone_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out=None, skip_in=0, use_bn=True, parametric=True):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        \n",
    "        self.parametric = parametric\n",
    "        ch_out = ch_in/2 if ch_out is None else ch_out\n",
    "        \n",
    "        if parametric:\n",
    "            self.up = nn.ConvTranspose2d(in_channels=ch_in,\n",
    "                                         out_channels=ch_out,\n",
    "                                         kernel_size=(4,4),\n",
    "                                         stride=2,\n",
    "                                         padding=1,\n",
    "                                         output_padding=0,\n",
    "                                         bias=(not use_bn))\n",
    "        else:\n",
    "            self.up = None\n",
    "            ch_in += skip_in\n",
    "            self.conv1 = nn.Conv2d(in_channels=ch_in,\n",
    "                                   out_channels=ch_out,\n",
    "                                   kernel_size=(3,3),\n",
    "                                   stride=1,\n",
    "                                   padding=1,\n",
    "                                   bias=(not use_bn))\n",
    "            \n",
    "        self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        conv2_in = ch_out if not parametric else ch_out + skip_in\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv2_in,\n",
    "                               out_channels=ch_out,\n",
    "                               kernel_size=(3,3),\n",
    "                               stride=1,\n",
    "                               padding=1,\n",
    "                               bias=(not use_bn))\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
    "        \n",
    "    def forward(self, x, skip_connection=None):\n",
    "        x = self.up(x) if self.parametric else F.interpolate(x, size=None, scale_factor=2, mode='bilinear', align_corners=None)\n",
    "        \n",
    "        if self.parametric:\n",
    "            x = self.bn1(x) if self.bn1 is not None else x\n",
    "            x = self.relu(x)\n",
    "        \n",
    "        if skip_connection is not None:\n",
    "            x = torch.cat([x, skip_connection], dim=1)\n",
    "            \n",
    "        if not self.parametric:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x) if self.bn1 is not None else x\n",
    "            x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x) if self.bn2 is not None else x\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone_name='densenet169',\n",
    "                 pretrained=True,\n",
    "                 encoder_freeze=True,\n",
    "                 input_size=(3, 480, 640),\n",
    "                 classes=1,\n",
    "                 decoder_filters=(256, 128, 64, 32, 16),\n",
    "                 parametric_upsampling=True,\n",
    "                 decoder_batchnorm=True):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # encoder\n",
    "        self.backbone_name = backbone_name\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.backbone, self.skip_features, self.bb_out_name = get_backbone(backbone_name, pretrained=True)\n",
    "        \n",
    "        skip_chs, bb_out_chs = self.infer_skip_channels(input_size)\n",
    "        \n",
    "        # decoder\n",
    "        self.upsample_blocks = nn.ModuleList()\n",
    "        \n",
    "        decoder_filters = decoder_filters[:len(self.skip_features)]\n",
    "        decoder_filters_in = [bb_out_chs] + list(decoder_filters[:-1])\n",
    "        \n",
    "        num_blocks = len(self.skip_features)\n",
    "        \n",
    "        for i, [filters_in, filters_out] in enumerate(zip(decoder_filters_in, decoder_filters)):\n",
    "            self.upsample_blocks.append(UpsampleBlock(filters_in, \n",
    "                                                      filters_out,\n",
    "                                                      skip_in=skip_chs[num_blocks-i-1],\n",
    "                                                      parametric=parametric_upsampling,\n",
    "                                                      use_bn=decoder_batchnorm))\n",
    "            \n",
    "        self.final_conv = nn.Conv2d(decoder_filters[-1], classes, kernel_size=(1,1))\n",
    "        \n",
    "        if encoder_freeze:\n",
    "            self.freeze_encoder()\n",
    "        \n",
    "        \n",
    "    def infer_skip_channels(self, input_size):\n",
    "        x = torch.unsqueeze(torch.zeros(input_size), 0)\n",
    "        \n",
    "        channels = [0]\n",
    "        \n",
    "        for name, child in self.backbone.named_children():\n",
    "            x = child(x)\n",
    "            if name in self.skip_features:\n",
    "                channels.append(x.shape[1])\n",
    "            if name == self.bb_out_name:\n",
    "                out_channels = x.shape[1]\n",
    "                break\n",
    "                \n",
    "        return channels, out_channels\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, *input):\n",
    "        x, features = self.forward_backbone(*input)\n",
    "        \n",
    "        for skip_name, upsample_block in zip(self.skip_features[::-1], self.upsample_blocks):\n",
    "            skip_features = features[skip_name]\n",
    "            x = upsample_block(x, skip_features)\n",
    "            \n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward_backbone(self, x):\n",
    "        features = {None:None} if None in self.skip_features else dict()\n",
    "        for name, child in self.backbone.named_children():\n",
    "            x = child(x)\n",
    "            if name in self.skip_features:\n",
    "                features[name] = x\n",
    "            if name == self.bb_out_name:\n",
    "                break\n",
    "                \n",
    "        return x, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(encoder_freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, dmap = next(iter(train_loader))\n",
    "output = net(img.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 1-output#torch.clip(1000.0/output, 10, 1000) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(dmap[0].numpy().transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(output[0].detach().cpu().numpy().transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_loss(gen_frames, gt_frames, alpha=1):\n",
    "\n",
    "    def gradient(x):\n",
    "        # idea from tf.image.image_gradients(image)\n",
    "        # https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/image_ops_impl.py#L3441-L3512\n",
    "        # x: (b,c,h,w), float32 or float64\n",
    "        # dx, dy: (b,c,h,w)\n",
    "\n",
    "        h_x = x.size()[-2]\n",
    "        w_x = x.size()[-1]\n",
    "        # gradient step=1\n",
    "        left = x\n",
    "        right = F.pad(x, [0, 1, 0, 0])[:, :, :, 1:]\n",
    "        top = x\n",
    "        bottom = F.pad(x, [0, 0, 0, 1])[:, :, 1:, :]\n",
    "\n",
    "        # dx, dy = torch.abs(right - left), torch.abs(bottom - top)\n",
    "        dx, dy = right - left, bottom - top \n",
    "        # dx will always have zeros in the last column, right-left\n",
    "        # dy will always have zeros in the last row,    bottom-top\n",
    "        dx[:, :, :, -1] = 0\n",
    "        dy[:, :, -1, :] = 0\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "    # gradient\n",
    "    gen_dx, gen_dy = gradient(gen_frames)\n",
    "    gt_dx, gt_dy = gradient(gt_frames)\n",
    "    \n",
    "    grad_diff_x = torch.abs(gt_dx - gen_dx)\n",
    "    grad_diff_y = torch.abs(gt_dy - gen_dy)\n",
    "\n",
    "    # condense into one tensor and avg\n",
    "    return torch.mean(grad_diff_x ** alpha + grad_diff_y ** alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = 0.0\n",
    "net.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    N = len(train_loader)\n",
    "    \n",
    "    for i, (image, depth) in enumerate(train_loader):\n",
    "        image, depth = image.cuda(), depth.cuda()\n",
    "        depth_n = 1000.0/depth\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(image)\n",
    "        \n",
    "        l_depth = l1_criterion(output, depth)\n",
    "        l_ssim = torch.clamp(1 - ssim(output, depth_n, val_range=1000.0/10.0) * 0.5, 0, 1)\n",
    "        l_grad = gradient_loss(output, depth_n)\n",
    "\n",
    "        loss = (0.1 * l_depth) + (1.0 * l_ssim) + (1.0 * l_grad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses += (1 / (i + 1)) * (loss.item()/image.size(0) - losses)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Loss {4} ({3})'\n",
    "            .format(epoch+1, i, N, loss, losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, dmap = next(iter(train_loader))\n",
    "output = net(img.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "k=1\n",
    "for sample_idx in range(2):\n",
    "    plt.subplot(2,2,k)\n",
    "    plt.imshow(dmap[sample_idx].numpy().transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    k += 1\n",
    "    plt.subplot(2,2,k)\n",
    "    plt.imshow(output[sample_idx].detach().cpu().numpy().transpose((1,2,0)), cmap='plasma', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    k += 1\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5LekAQCDoT1nCawFswPOs",
   "include_colab_link": true,
   "name": "Monocular_Depth_Estimate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
